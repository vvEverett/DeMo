defaults:
  - datamodule: av2
  - model: model_forecast_moe  # Use MoE predictor with 6 experts

hydra:
  run:
    dir: outputs/${output}/${now:%Y%m%d-%H%M%S}

seed: 2024
monitor: val_minFDE6
save_top_k: 10

# Weights & Biases configuration
wandb_project: DeMo  # Your W&B project name
wandb_run_name:  # Optional: custom run name (auto-generated if not set)

data_root:
checkpoint:  # Leave empty for training from scratch
pretrained_weights:  # Path to MLP baseline checkpoint to initialize encoder (optional)
output: train_moe_${datamodule.name}_${datamodule.phase}_${model.name}_${model.phase}

# trainer
num_workers: 4
gpus: 6  # Use 6 GPUs for training
sync_bn: true
batch_size: 16
epochs: 60
warmup_epochs: 10

# optimizer
lr: 0.00225  # Learning rate scaled for 6 GPUs
weight_decay: 1e-2  
gradient_clip_val: 5
gradient_clip_algorithm: norm

# MoE-specific hyperparameters to prevent expert collapse
# NOTE: These are the PRIMARY configuration values for training.
# Adjust these values to control expert specialization behavior.
aux_loss_weight: 1.0  # Increased from 0.01 for stronger load balancing
diversity_loss_weight: 1.0  # Increased from 0.1 for better expert specialization

limit_train_batches:
limit_val_batches:
limit_test_batches:
log_model: all
test: false
